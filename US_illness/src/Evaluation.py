import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import torch
import warnings

warnings.filterwarnings('ignore')

plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False
sns.set_style("whitegrid")


class ModelEvaluator:
    """
    å®Œæ•´çš„æ¨¡å‹è¯„ä¼°å·¥å…·ç±»
    é€‚ç”¨äºæ‚¨çš„å›å½’é¢„æµ‹é—®é¢˜
    """

    def __init__(self, model, trainer=None, device=None):
        """
        åˆå§‹åŒ–è¯„ä¼°å™¨

        Parameters:
        -----------
        model : torch.nn.Module
            è®­ç»ƒå¥½çš„PyTorchæ¨¡å‹
        trainer : ModelTrainer, optional
            æ‚¨çš„è®­ç»ƒå™¨å¯¹è±¡ï¼Œç”¨äºè·å–è®­ç»ƒå†å²
        device : torch.device, optional
            è®¾å¤‡ï¼ˆCPU/GPUï¼‰
        """
        self.model = model
        self.trainer = trainer
        self.device = device if device else torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model.to(self.device)
        self.model.eval()

        self.metrics = {}
        self.predictions = {}

    def predict(self, X):
        """ç”Ÿæˆé¢„æµ‹"""
        if not torch.is_tensor(X):
            X = torch.FloatTensor(X)

        X = X.to(self.device)
        self.model.eval()

        with torch.no_grad():
            y_pred = self.model(X)

        return y_pred.cpu().numpy()

    def compute_metrics(self, y_true, y_pred, prefix='val'):
        """è®¡ç®—æ‰€æœ‰è¯„ä¼°æŒ‡æ ‡"""
        y_true = np.array(y_true).flatten()
        y_pred = np.array(y_pred).flatten()

        # åŸºæœ¬å›å½’æŒ‡æ ‡
        mae = mean_absolute_error(y_true, y_pred)
        mse = mean_squared_error(y_true, y_pred)
        rmse = np.sqrt(mse)
        r2 = r2_score(y_true, y_pred)

        # ç™¾åˆ†æ¯”è¯¯å·®
        absolute_percentage_error = np.abs((y_true - y_pred) / (np.abs(y_true) + 1e-10)) * 100
        mape = np.mean(absolute_percentage_error)

        # å¯¹ç§°MAPE
        smape = 100 * np.mean(2 * np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred) + 1e-10))

        # æœ€å¤§æœ€å°è¯¯å·®
        max_error = np.max(np.abs(y_true - y_pred))
        min_error = np.min(np.abs(y_true - y_pred))

        # è¯¯å·®ç»Ÿè®¡
        errors = y_true - y_pred
        mean_error = np.mean(errors)
        std_error = np.std(errors)
        median_error = np.median(np.abs(errors))

        # ç²¾åº¦æ¯”ä¾‹
        thresholds = [1, 5, 10, 15, 20]
        accuracy_rates = {}
        for threshold in thresholds:
            accuracy_rates[f'within_{threshold}pct'] = np.mean(absolute_percentage_error <= threshold) * 100
            accuracy_rates[f'within_{threshold}_abs'] = np.mean(np.abs(errors) <= threshold) * 100

        metrics = {
            f'{prefix}_mae': mae,
            f'{prefix}_mse': mse,
            f'{prefix}_rmse': rmse,
            f'{prefix}_r2': r2,
            f'{prefix}_mape': mape,
            f'{prefix}_smape': smape,
            f'{prefix}_max_error': max_error,
            f'{prefix}_min_error': min_error,
            f'{prefix}_mean_error': mean_error,
            f'{prefix}_std_error': std_error,
            f'{prefix}_median_abs_error': median_error,
            f'{prefix}_errors': errors,
            f'{prefix}_percentage_errors': absolute_percentage_error,
            f'{prefix}_y_true': y_true,
            f'{prefix}_y_pred': y_pred,
        }

        metrics.update(accuracy_rates)
        self.metrics.update(metrics)

        return metrics

    def compare_with_baselines(self, y_true, X_data=None):
        """ä¸åŸºå‡†æ¨¡å‹æ¯”è¾ƒ"""
        y_true = np.array(y_true).flatten()

        baselines = {}

        # åŸºå‡†1ï¼šå‡å€¼é¢„æµ‹
        baseline_mean = np.mean(y_true)
        baselines['mean'] = {
            'mae': mean_absolute_error(y_true, np.full_like(y_true, baseline_mean)),
            'rmse': np.sqrt(mean_squared_error(y_true, np.full_like(y_true, baseline_mean))),
            'predictions': np.full_like(y_true, baseline_mean)
        }

        # åŸºå‡†2ï¼šä¸­ä½æ•°é¢„æµ‹
        baseline_median = np.median(y_true)
        baselines['median'] = {
            'mae': mean_absolute_error(y_true, np.full_like(y_true, baseline_median)),
            'rmse': np.sqrt(mean_squared_error(y_true, np.full_like(y_true, baseline_median))),
            'predictions': np.full_like(y_true, baseline_median)
        }

        # åŸºå‡†3ï¼šæœ€åä¸€ä¸ªå·²çŸ¥å€¼ï¼ˆé€‚ç”¨äºæ—¶é—´åºåˆ—ï¼‰
        if X_data is not None and hasattr(X_data, 'shape'):
            # å‡è®¾æœ€åä¸€ä¸ªç‰¹å¾æ˜¯y_bï¼ˆæ—¶é—´ç‚¹bçš„å€¼ï¼‰
            try:
                if torch.is_tensor(X_data):
                    y_b = X_data[:, -1].cpu().numpy()  # å‡è®¾æœ€åä¸€åˆ—æ˜¯y_b
                else:
                    y_b = X_data[:, -1]  # å‡è®¾æœ€åä¸€åˆ—æ˜¯y_b

                baselines['last_value'] = {
                    'mae': mean_absolute_error(y_true, y_b),
                    'rmse': np.sqrt(mean_squared_error(y_true, y_b)),
                    'predictions': y_b
                }
            except:
                pass

        # åŸºå‡†4ï¼šçº¿æ€§å¤–æ¨ï¼ˆå¦‚æœçŸ¥é“y_aå’Œy_bï¼‰
        if X_data is not None and X_data.shape[1] >= 2:
            try:
                if torch.is_tensor(X_data):
                    y_a = X_data[:, -2].cpu().numpy()
                    y_b = X_data[:, -1].cpu().numpy()
                else:
                    y_a = X_data[:, -2]
                    y_b = X_data[:, -1]

                # çº¿æ€§å¤–æ¨ï¼šy_c = y_b + (y_b - y_a)
                linear_extrapolation = 2 * y_b - y_a
                baselines['linear_extrapolation'] = {
                    'mae': mean_absolute_error(y_true, linear_extrapolation),
                    'rmse': np.sqrt(mean_squared_error(y_true, linear_extrapolation)),
                    'predictions': linear_extrapolation
                }
            except:
                pass

        return baselines

    def plot_training_history(self):
        """ç»˜åˆ¶è®­ç»ƒå†å²æ›²çº¿"""
        if self.trainer is None:
            print("è­¦å‘Šï¼šæœªæä¾›trainerå¯¹è±¡ï¼Œæ— æ³•ç»˜åˆ¶è®­ç»ƒå†å²")
            return

        fig, axes = plt.subplots(2, 2, figsize=(15, 12))

        # 1. æŸå¤±æ›²çº¿
        axes[0, 0].plot(self.trainer.train_losses, label='è®­ç»ƒæŸå¤±', linewidth=2, alpha=0.8)
        axes[0, 0].plot(self.trainer.val_losses, label='éªŒè¯æŸå¤±', linewidth=2, alpha=0.8)
        axes[0, 0].set_xlabel('è®­ç»ƒè½®æ¬¡ (Epoch)', fontsize=12)
        axes[0, 0].set_ylabel('æŸå¤± (MSE)', fontsize=12)
        axes[0, 0].set_title('è®­ç»ƒå’ŒéªŒè¯æŸå¤±æ›²çº¿', fontsize=14, fontweight='bold')
        axes[0, 0].legend(fontsize=11)
        axes[0, 0].grid(True, alpha=0.3)

        # æ ‡è®°æœ€ä½³epoch
        if hasattr(self.trainer, 'best_val_loss'):
            best_idx = np.argmin(self.trainer.val_losses)
            axes[0, 0].axvline(x=best_idx, color='red', linestyle='--', alpha=0.7,
                               label=f'æœ€ä½³epoch: {best_idx + 1}')
            axes[0, 0].scatter(best_idx, self.trainer.val_losses[best_idx],
                               color='red', s=100, zorder=5)

        # 2. MAEæ›²çº¿
        axes[0, 1].plot(self.trainer.train_maes, label='è®­ç»ƒMAE', linewidth=2, alpha=0.8)
        axes[0, 1].plot(self.trainer.val_maes, label='éªŒè¯MAE', linewidth=2, alpha=0.8)
        axes[0, 1].set_xlabel('è®­ç»ƒè½®æ¬¡ (Epoch)', fontsize=12)
        axes[0, 1].set_ylabel('MAE', fontsize=12)
        axes[0, 1].set_title('è®­ç»ƒå’ŒéªŒè¯MAEæ›²çº¿', fontsize=14, fontweight='bold')
        axes[0, 1].legend(fontsize=11)
        axes[0, 1].grid(True, alpha=0.3)

        # 3. æŸå¤±å¯¹æ•°å›¾
        axes[1, 0].semilogy(self.trainer.train_losses, label='è®­ç»ƒæŸå¤±', linewidth=2, alpha=0.8)
        axes[1, 0].semilogy(self.trainer.val_losses, label='éªŒè¯æŸå¤±', linewidth=2, alpha=0.8)
        axes[1, 0].set_xlabel('è®­ç»ƒè½®æ¬¡ (Epoch)', fontsize=12)
        axes[1, 0].set_ylabel('æŸå¤± (å¯¹æ•°å°ºåº¦)', fontsize=12)
        axes[1, 0].set_title('æŸå¤±æ›²çº¿ï¼ˆå¯¹æ•°å°ºåº¦ï¼‰', fontsize=14, fontweight='bold')
        axes[1, 0].legend(fontsize=11)
        axes[1, 0].grid(True, alpha=0.3)

        # 4. è¿‡æ‹Ÿåˆåˆ†æ
        if len(self.trainer.train_losses) > 0 and len(self.trainer.val_losses) > 0:
            overfitting_ratio = np.array(self.trainer.val_losses) / np.array(self.trainer.train_losses)
            axes[1, 1].plot(overfitting_ratio, label='éªŒè¯æŸå¤±/è®­ç»ƒæŸå¤±', linewidth=2,
                            color='purple', alpha=0.8)
            axes[1, 1].axhline(y=1.0, color='green', linestyle='--', alpha=0.7, label='ç†æƒ³çº¿')
            axes[1, 1].axhline(y=1.5, color='red', linestyle='--', alpha=0.7, label='è¿‡æ‹Ÿåˆè­¦æˆ’çº¿')
            axes[1, 1].set_xlabel('è®­ç»ƒè½®æ¬¡ (Epoch)', fontsize=12)
        axes[1, 1].set_ylabel('æŸå¤±æ¯”ä¾‹', fontsize=12)
        axes[1, 1].set_title('è¿‡æ‹Ÿåˆåˆ†æ', fontsize=14, fontweight='bold')
        axes[1, 1].legend(fontsize=11)
        axes[1, 1].grid(True, alpha=0.3)

        plt.suptitle('æ¨¡å‹è®­ç»ƒå†å²åˆ†æ', fontsize=16, fontweight='bold', y=1.02)
        plt.tight_layout()
        plt.show()

    def plot_prediction_analysis(self, y_true, y_pred):
        """ç»˜åˆ¶é¢„æµ‹åˆ†æå›¾"""
        y_true = np.array(y_true).flatten()
        y_pred = np.array(y_pred).flatten()

        fig = plt.figure(figsize=(20, 16))

        # 1. é¢„æµ‹ vs çœŸå®å€¼æ•£ç‚¹å›¾
        ax1 = plt.subplot(3, 3, 1)
        scatter = ax1.scatter(y_true, y_pred, alpha=0.6, s=30, c=np.abs(y_true - y_pred),
                              cmap='viridis', edgecolors='black', linewidth=0.5)
        max_val = max(y_true.max(), y_pred.max())
        min_val = min(y_true.min(), y_pred.min())
        ax1.plot([min_val, max_val], [min_val, max_val], 'r--', lw=3, label='å®Œç¾é¢„æµ‹çº¿')
        ax1.set_xlabel('çœŸå®å€¼', fontsize=12)
        ax1.set_ylabel('é¢„æµ‹å€¼', fontsize=12)
        ax1.set_title('é¢„æµ‹å€¼ vs çœŸå®å€¼', fontsize=14, fontweight='bold')
        ax1.legend(fontsize=11)
        ax1.grid(True, alpha=0.3)

        # æ·»åŠ é¢œè‰²æ¡
        cbar = plt.colorbar(scatter, ax=ax1)
        cbar.set_label('ç»å¯¹è¯¯å·®', fontsize=11)

        # 2. æ®‹å·®å›¾
        ax2 = plt.subplot(3, 3, 2)
        residuals = y_true - y_pred
        ax2.scatter(y_pred, residuals, alpha=0.6, s=30, c=np.abs(residuals),
                    cmap='coolwarm', edgecolors='black', linewidth=0.5)
        ax2.axhline(y=0, color='r', linestyle='--', lw=3)
        ax2.set_xlabel('é¢„æµ‹å€¼', fontsize=12)
        ax2.set_ylabel('æ®‹å·® (çœŸå®å€¼ - é¢„æµ‹å€¼)', fontsize=12)
        ax2.set_title('æ®‹å·®å›¾', fontsize=14, fontweight='bold')
        ax2.grid(True, alpha=0.3)

        # 3. æ®‹å·®åˆ†å¸ƒ
        ax3 = plt.subplot(3, 3, 3)
        n, bins, patches = ax3.hist(residuals, bins=40, edgecolor='black',
                                    alpha=0.7, color='skyblue')
        ax3.axvline(x=0, color='r', linestyle='--', lw=3)
        ax3.set_xlabel('æ®‹å·®', fontsize=12)
        ax3.set_ylabel('é¢‘æ•°', fontsize=12)
        ax3.set_title('æ®‹å·®åˆ†å¸ƒç›´æ–¹å›¾', fontsize=14, fontweight='bold')

        # æ·»åŠ æ­£æ€åˆ†å¸ƒæ›²çº¿
        mu, std = stats.norm.fit(residuals)
        xmin, xmax = ax3.get_xlim()
        x = np.linspace(xmin, xmax, 100)
        p = stats.norm.pdf(x, mu, std)
        ax3.plot(x, p * len(residuals) * (bins[1] - bins[0]), 'r-', lw=2,
                 label=f'æ­£æ€åˆ†å¸ƒæ‹Ÿåˆ\nÎ¼={mu:.2f}, Ïƒ={std:.2f}')
        ax3.legend(fontsize=10)

        # 4. ç™¾åˆ†æ¯”è¯¯å·®åˆ†å¸ƒ
        ax4 = plt.subplot(3, 3, 4)
        percentage_errors = np.abs(residuals / (np.abs(y_true) + 1e-10)) * 100
        n, bins, patches = ax4.hist(percentage_errors, bins=40, edgecolor='black',
                                    alpha=0.7, color='lightcoral')

        # æ·»åŠ ç™¾åˆ†æ¯”çº¿
        for threshold in [10, 20, 30]:
            color = 'green' if threshold == 10 else 'orange' if threshold == 20 else 'red'
            ax4.axvline(x=threshold, color=color, linestyle='--', lw=2,
                        label=f'{threshold}%è¯¯å·®çº¿')

        ax4.set_xlabel('ç»å¯¹ç™¾åˆ†æ¯”è¯¯å·® (%)', fontsize=12)
        ax4.set_ylabel('é¢‘æ•°', fontsize=12)
        ax4.set_title('ç™¾åˆ†æ¯”è¯¯å·®åˆ†å¸ƒ', fontsize=14, fontweight='bold')
        ax4.legend(fontsize=10)

        # 5. ç´¯ç§¯è¯¯å·®åˆ†å¸ƒ
        ax5 = plt.subplot(3, 3, 5)
        sorted_abs_errors = np.sort(np.abs(residuals))
        cumulative_prop = np.arange(1, len(sorted_abs_errors) + 1) / len(sorted_abs_errors)

        ax5.plot(sorted_abs_errors, cumulative_prop * 100, lw=3, color='darkblue')

        # æ ‡è®°å…³é”®ç‚¹
        for pct in [50, 80, 90, 95]:
            idx = int(pct / 100 * len(sorted_abs_errors)) - 1
            if idx >= 0:
                ax5.scatter(sorted_abs_errors[idx], pct, color='red', s=100, zorder=5)
                ax5.annotate(f'{pct}%: {sorted_abs_errors[idx]:.2f}',
                             xy=(sorted_abs_errors[idx], pct),
                             xytext=(10, 10), textcoords='offset points',
                             fontsize=10, bbox=dict(boxstyle="round,pad=0.3",
                                                    facecolor="yellow", alpha=0.7))

        ax5.set_xlabel('ç»å¯¹è¯¯å·®', fontsize=12)
        ax5.set_ylabel('ç´¯ç§¯ç™¾åˆ†æ¯” (%)', fontsize=12)
        ax5.set_title('ç´¯ç§¯è¯¯å·®åˆ†å¸ƒ', fontsize=14, fontweight='bold')
        ax5.grid(True, alpha=0.3)

        # 6. QQå›¾ï¼ˆæ­£æ€æ€§æ£€éªŒï¼‰
        ax6 = plt.subplot(3, 3, 6)
        (osm, osr), (slope, intercept, r) = stats.probplot(residuals, dist="norm", plot=None)
        ax6.plot(osm, osr, 'o', alpha=0.6, markersize=6, markeredgecolor='black',
                 markeredgewidth=0.5)
        ax6.plot(osm, slope * osm + intercept, 'r-', lw=3,
                 label=f'æ‹Ÿåˆçº¿ (R={r:.3f})')
        ax6.set_xlabel('ç†è®ºåˆ†ä½æ•°', fontsize=12)
        ax6.set_ylabel('æ ·æœ¬åˆ†ä½æ•°', fontsize=12)
        ax6.set_title('QQå›¾ - æ®‹å·®æ­£æ€æ€§æ£€éªŒ', fontsize=14, fontweight='bold')
        ax6.legend(fontsize=10)
        ax6.grid(True, alpha=0.3)

        # 7. è¯¯å·®éšæ—¶é—´/é¡ºåºå˜åŒ–
        ax7 = plt.subplot(3, 3, 7)
        ax7.plot(np.abs(residuals), alpha=0.7, lw=2, color='darkgreen')
        ax7.axhline(y=np.mean(np.abs(residuals)), color='red',
                    linestyle='--', lw=2, label=f'å¹³å‡ç»å¯¹è¯¯å·®: {np.mean(np.abs(residuals)):.2f}')
        ax7.fill_between(range(len(residuals)), 0, np.abs(residuals),
                         alpha=0.3, color='lightgreen')
        ax7.set_xlabel('æ ·æœ¬åºå·', fontsize=12)
        ax7.set_ylabel('ç»å¯¹è¯¯å·®', fontsize=12)
        ax7.set_title('è¯¯å·®éšæ—¶é—´/é¡ºåºå˜åŒ–', fontsize=14, fontweight='bold')
        ax7.legend(fontsize=10)
        ax7.grid(True, alpha=0.3)

        # 8. è¯¯å·®ç®±çº¿å›¾
        ax8 = plt.subplot(3, 3, 8)
        bp = ax8.boxplot([residuals, np.abs(residuals), percentage_errors],
                         labels=['æ®‹å·®', 'ç»å¯¹è¯¯å·®', 'ç™¾åˆ†æ¯”è¯¯å·®(%)'],
                         patch_artist=True,
                         medianprops=dict(color='black', linewidth=2),
                         boxprops=dict(facecolor='lightblue', alpha=0.7))

        # è®¾ç½®é¢œè‰²
        colors = ['lightblue', 'lightgreen', 'lightcoral']
        for patch, color in zip(bp['boxes'], colors):
            patch.set_facecolor(color)

        ax8.set_ylabel('å€¼', fontsize=12)
        ax8.set_title('è¯¯å·®ç»Ÿè®¡ç®±çº¿å›¾', fontsize=14, fontweight='bold')
        ax8.grid(True, alpha=0.3, axis='y')

        # 9. é¢„æµ‹å€¼åˆ†å¸ƒå¯¹æ¯”
        ax9 = plt.subplot(3, 3, 9)
        bins = np.linspace(min(min(y_true), min(y_pred)),
                           max(max(y_true), max(y_pred)), 30)
        ax9.hist(y_true, bins=bins, alpha=0.5, label='çœŸå®å€¼', color='blue',
                 edgecolor='black', density=True)
        ax9.hist(y_pred, bins=bins, alpha=0.5, label='é¢„æµ‹å€¼', color='red',
                 edgecolor='black', density=True)
        ax9.set_xlabel('å€¼', fontsize=12)
        ax9.set_ylabel('å¯†åº¦', fontsize=12)
        ax9.set_title('çœŸå®å€¼ä¸é¢„æµ‹å€¼åˆ†å¸ƒå¯¹æ¯”', fontsize=14, fontweight='bold')
        ax9.legend(fontsize=11)

        plt.suptitle('æ¨¡å‹é¢„æµ‹åˆ†ææŠ¥å‘Š', fontsize=18, fontweight='bold', y=1.02)
        plt.tight_layout()
        plt.show()

        return residuals, percentage_errors

    def plot_baseline_comparison(self, y_true, y_pred, baselines):
        """ç»˜åˆ¶ä¸åŸºå‡†æ¨¡å‹çš„æ¯”è¾ƒ"""
        y_true = np.array(y_true).flatten()
        y_pred = np.array(y_pred).flatten()

        models = ['æ‚¨çš„æ¨¡å‹']
        mae_scores = [mean_absolute_error(y_true, y_pred)]
        rmse_scores = [np.sqrt(mean_squared_error(y_true, y_pred))]

        for name, baseline in baselines.items():
            models.append(name)
            mae_scores.append(baseline['mae'])
            rmse_scores.append(baseline['rmse'])

        fig, axes = plt.subplots(1, 2, figsize=(14, 6))

        # MAEæ¯”è¾ƒ
        bars1 = axes[0].bar(models, mae_scores, color=['blue'] + ['gray'] * len(baselines),
                            alpha=0.7, edgecolor='black')
        axes[0].set_xlabel('æ¨¡å‹', fontsize=12)
        axes[0].set_ylabel('MAE', fontsize=12)
        axes[0].set_title('MAEæ¯”è¾ƒ (è¶Šä½è¶Šå¥½)', fontsize=14, fontweight='bold')
        axes[0].grid(True, alpha=0.3, axis='y')

        # åœ¨æŸ±å­ä¸Šæ·»åŠ æ•°å€¼
        for i, (bar, score) in enumerate(zip(bars1, mae_scores)):
            axes[0].text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.01,
                         f'{score:.3f}', ha='center', va='bottom', fontsize=10)

        # è®¡ç®—æå‡ç™¾åˆ†æ¯”
        improvement = []
        for i, score in enumerate(mae_scores[1:], 1):
            improv_pct = (mae_scores[0] - score) / score * 100
            improvement.append(improv_pct)
            axes[0].text(i, mae_scores[i] * 1.05,
                         f'{improv_pct:+.1f}%' if i > 0 else '',
                         ha='center', va='bottom', fontsize=9,
                         bbox=dict(boxstyle="round,pad=0.3", facecolor="yellow", alpha=0.7))

        # RMSEæ¯”è¾ƒ
        bars2 = axes[1].bar(models, rmse_scores, color=['blue'] + ['gray'] * len(baselines),
                            alpha=0.7, edgecolor='black')
        axes[1].set_xlabel('æ¨¡å‹', fontsize=12)
        axes[1].set_ylabel('RMSE', fontsize=12)
        axes[1].set_title('RMSEæ¯”è¾ƒ (è¶Šä½è¶Šå¥½)', fontsize=14, fontweight='bold')
        axes[1].grid(True, alpha=0.3, axis='y')

        # åœ¨æŸ±å­ä¸Šæ·»åŠ æ•°å€¼
        for i, (bar, score) in enumerate(zip(bars2, rmse_scores)):
            axes[1].text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.01,
                         f'{score:.3f}', ha='center', va='bottom', fontsize=10)

        plt.suptitle('æ¨¡å‹æ€§èƒ½ä¸åŸºå‡†å¯¹æ¯”', fontsize=16, fontweight='bold')
        plt.tight_layout()
        plt.show()

        return models, mae_scores, rmse_scores

    def generate_report(self, X_train=None, y_train=None, X_val=None, y_val=None,
                        X_test=None, y_test=None, feature_names=None):
        """ç”Ÿæˆå®Œæ•´çš„è¯„ä¼°æŠ¥å‘Š"""
        print("=" * 80)
        print("ğŸ¤– æœºå™¨å­¦ä¹ æ¨¡å‹è¯„ä¼°æŠ¥å‘Š")
        print("=" * 80)

        # å­˜å‚¨æ‰€æœ‰ç»“æœ
        results = {}

        # 1. è®­ç»ƒå†å²åˆ†æ
        if self.trainer:
            print("\nğŸ“Š 1. è®­ç»ƒå†å²åˆ†æ")
            print("-" * 40)

            train_losses = np.array(self.trainer.train_losses)
            val_losses = np.array(self.trainer.val_losses)

            best_epoch = np.argmin(val_losses) + 1
            final_train_loss = train_losses[-1]
            final_val_loss = val_losses[-1]

            print(f"   æ€»è®­ç»ƒè½®æ¬¡: {len(train_losses)}")
            print(f"   æœ€ä½³éªŒè¯è½®æ¬¡: {best_epoch}")
            print(f"   æœ€ç»ˆè®­ç»ƒæŸå¤±: {final_train_loss:.6f}")
            print(f"   æœ€ç»ˆéªŒè¯æŸå¤±: {final_val_loss:.6f}")
            print(f"   æœ€ä½³éªŒè¯æŸå¤±: {np.min(val_losses):.6f}")

            # æ”¶æ•›åˆ†æ
            if len(val_losses) >= 10:
                last_10_std = np.std(val_losses[-10:])
                print(f"   æœ€å10è½®æŸå¤±æ ‡å‡†å·®: {last_10_std:.6f}")
                if last_10_std < 0.001 * np.mean(val_losses):
                    print("   âœ… æ¨¡å‹å·²æ”¶æ•›")
                else:
                    print("   âš ï¸  æ¨¡å‹å¯èƒ½æœªå®Œå…¨æ”¶æ•›")

            # è¿‡æ‹Ÿåˆåˆ†æ
            overfitting_ratio = final_val_loss / final_train_loss
            print(f"   éªŒè¯/è®­ç»ƒæŸå¤±æ¯”: {overfitting_ratio:.3f}")
            if overfitting_ratio > 1.5:
                print("   âš ï¸  è­¦å‘Šï¼šå¯èƒ½è¿‡æ‹Ÿåˆ")
            elif overfitting_ratio < 1.1:
                print("   âœ… è‰¯å¥½ï¼šæ¬ æ‹Ÿåˆé£é™©ä½")
            else:
                print("   âš ï¸  æ³¨æ„ï¼šæœ‰ä¸€å®šè¿‡æ‹Ÿåˆè¿¹è±¡")

            results['training_history'] = {
                'best_epoch': best_epoch,
                'final_train_loss': final_train_loss,
                'final_val_loss': final_val_loss,
                'best_val_loss': np.min(val_losses),
                'overfitting_ratio': overfitting_ratio
            }

        # 2. éªŒè¯é›†è¯„ä¼°
        if X_val is not None and y_val is not None:
            print("\nğŸ“ˆ 2. éªŒè¯é›†æ€§èƒ½è¯„ä¼°")
            print("-" * 40)

            # ç”Ÿæˆé¢„æµ‹
            y_val_pred = self.predict(X_val)
            y_val_true = np.array(y_val).flatten()

            # è®¡ç®—æŒ‡æ ‡
            val_metrics = self.compute_metrics(y_val_true, y_val_pred, 'val')

            print(f"   MAE: {val_metrics['val_mae']:.4f}")
            print(f"   RMSE: {val_metrics['val_rmse']:.4f}")
            print(f"   RÂ²: {val_metrics['val_r2']:.4f}")
            print(f"   MAPE: {val_metrics['val_mape']:.2f}%")
            print(f"   SMAPE: {val_metrics['val_smape']:.2f}%")
            print(f"   æœ€å¤§è¯¯å·®: {val_metrics['val_max_error']:.4f}")
            print(f"   è¯¯å·®æ ‡å‡†å·®: {val_metrics['val_std_error']:.4f}")

            # ç²¾åº¦ç»Ÿè®¡
            print(f"\n   ğŸ“Š é¢„æµ‹ç²¾åº¦:")
            for threshold in [5, 10, 15, 20]:
                key = f'val_within_{threshold}pct'
                if key in val_metrics:
                    print(f"     è¯¯å·®åœ¨Â±{threshold}%ä»¥å†…: {val_metrics[key]:.1f}%")

            results['validation_metrics'] = val_metrics

            # ç»˜åˆ¶é¢„æµ‹åˆ†æå›¾
            print("\n   ğŸ“‰ ç»˜åˆ¶é¢„æµ‹åˆ†æå›¾...")
            residuals, percentage_errors = self.plot_prediction_analysis(y_val_true, y_val_pred)

            # ä¸åŸºå‡†æ¯”è¾ƒ
            print("\n   ğŸ“Š ä¸åŸºå‡†æ¨¡å‹æ¯”è¾ƒ...")
            baselines = self.compare_with_baselines(y_val_true, X_val)
            self.plot_baseline_comparison(y_val_true, y_val_pred, baselines)

            # æ‰“å°åŸºå‡†æ¯”è¾ƒ
            print("\n     åŸºå‡†æ¨¡å‹æ€§èƒ½:")
            for name, baseline in baselines.items():
                print(f"     {name}: MAE={baseline['mae']:.4f}, "
                      f"RMSE={baseline['rmse']:.4f}")

                # è®¡ç®—æå‡
                improv_mae = (baseline['mae'] - val_metrics['val_mae']) / baseline['mae'] * 100
                improv_rmse = (baseline['rmse'] - val_metrics['val_rmse']) / baseline['rmse'] * 100
                print(f"       ç›¸å¯¹æå‡: MAE={improv_mae:+.1f}%, RMSE={improv_rmse:+.1f}%")

        # 3. æµ‹è¯•é›†è¯„ä¼°ï¼ˆå¦‚æœæœ‰ï¼‰
        if X_test is not None and y_test is not None:
            print("\nğŸ§ª 3. æµ‹è¯•é›†æ€§èƒ½è¯„ä¼°")
            print("-" * 40)

            y_test_pred = self.predict(X_test)
            y_test_true = np.array(y_test).flatten()

            test_metrics = self.compute_metrics(y_test_true, y_test_pred, 'test')

            print(f"   MAE: {test_metrics['test_mae']:.4f}")
            print(f"   RMSE: {test_metrics['test_rmse']:.4f}")
            print(f"   RÂ²: {test_metrics['test_r2']:.4f}")
            print(f"   MAPE: {test_metrics['test_mape']:.2f}%")

            # ä¸éªŒè¯é›†æ¯”è¾ƒ
            if 'validation_metrics' in results:
                print(f"\n   ğŸ”„ ä¸éªŒè¯é›†æ¯”è¾ƒ:")
                mae_diff = test_metrics['test_mae'] - results['validation_metrics']['val_mae']
                r2_diff = test_metrics['test_r2'] - results['validation_metrics']['val_r2']

                print(f"     MAEå˜åŒ–: {mae_diff:+.4f}")
                print(f"     RÂ²å˜åŒ–: {r2_diff:+.4f}")

                if abs(mae_diff) > 0.1 * results['validation_metrics']['val_mae']:
                    print("     âš ï¸  æ³¨æ„ï¼šæµ‹è¯•é›†ä¸éªŒè¯é›†æ€§èƒ½å·®å¼‚è¾ƒå¤§")
                else:
                    print("     âœ… è‰¯å¥½ï¼šæµ‹è¯•é›†ä¸éªŒè¯é›†æ€§èƒ½ä¸€è‡´")

            results['test_metrics'] = test_metrics

        # 4. è®­ç»ƒé›†è¯„ä¼°ï¼ˆæ£€æŸ¥è¿‡æ‹Ÿåˆï¼‰
        if X_train is not None and y_train is not None:
            print("\nğŸ“ 4. è®­ç»ƒé›†æ€§èƒ½ï¼ˆæ£€æŸ¥è¿‡æ‹Ÿåˆï¼‰")
            print("-" * 40)

            y_train_pred = self.predict(X_train)
            y_train_true = np.array(y_train).flatten()

            train_metrics = self.compute_metrics(y_train_true, y_train_pred, 'train')

            print(f"   MAE: {train_metrics['train_mae']:.4f}")
            print(f"   RÂ²: {train_metrics['train_r2']:.4f}")

            if 'validation_metrics' in results:
                gap_mae = train_metrics['train_mae'] - results['validation_metrics']['val_mae']
                gap_r2 = results['validation_metrics']['val_r2'] - train_metrics['train_r2']

                print(f"\n   ğŸ” è®­ç»ƒé›†-éªŒè¯é›†å·®è·:")
                print(f"     MAEå·®è·: {gap_mae:+.4f} "
                      f"(è´Ÿå€¼è¡¨ç¤ºè¿‡æ‹Ÿåˆ)")
                print(f"     RÂ²å·®è·: {gap_r2:+.4f} "
                      f"(æ­£å€¼è¡¨ç¤ºè¿‡æ‹Ÿåˆ)")

                if gap_mae < -0.1 * results['validation_metrics']['val_mae']:
                    print("     âš ï¸  å¯èƒ½è¿‡æ‹Ÿåˆï¼šè®­ç»ƒé›†æ€§èƒ½æ˜æ˜¾ä¼˜äºéªŒè¯é›†")
                elif gap_mae > 0.1 * results['validation_metrics']['val_mae']:
                    print("     âš ï¸  å¯èƒ½æ¬ æ‹Ÿåˆï¼šè®­ç»ƒé›†æ€§èƒ½ä¸å¦‚éªŒè¯é›†")
                else:
                    print("     âœ… è‰¯å¥½ï¼šè®­ç»ƒé›†å’ŒéªŒè¯é›†æ€§èƒ½å¹³è¡¡")

            results['train_metrics'] = train_metrics

        # 5. æ€§èƒ½æ€»ç»“
        print("\n" + "=" * 80)
        print("ğŸ“‹ 5. æ€§èƒ½æ€»ç»“ä¸å»ºè®®")
        print("=" * 80)

        # æ”¶é›†å…³é”®æŒ‡æ ‡
        key_metrics = {}
        if 'validation_metrics' in results:
            key_metrics.update({
                'val_mae': results['validation_metrics']['val_mae'],
                'val_r2': results['validation_metrics']['val_r2'],
                'val_mape': results['validation_metrics']['val_mape']
            })

        if 'test_metrics' in results:
            key_metrics.update({
                'test_mae': results['test_metrics']['test_mae'],
                'test_r2': results['test_metrics']['test_r2']
            })

        # åˆ¤æ–­æ¨¡å‹è´¨é‡
        recommendations = []

        if 'val_r2' in key_metrics:
            r2 = key_metrics['val_r2']
            if r2 >= 0.9:
                recommendations.append("âœ… æ¨¡å‹è§£é‡ŠåŠ›æå¼º (RÂ² â‰¥ 0.9)")
            elif r2 >= 0.7:
                recommendations.append("âœ… æ¨¡å‹è§£é‡ŠåŠ›è‰¯å¥½ (0.7 â‰¤ RÂ² < 0.9)")
            elif r2 >= 0.5:
                recommendations.append("âš ï¸  æ¨¡å‹è§£é‡ŠåŠ›ä¸€èˆ¬ (0.5 â‰¤ RÂ² < 0.7)ï¼Œå¯è€ƒè™‘ä¼˜åŒ–ç‰¹å¾")
            else:
                recommendations.append("âŒ æ¨¡å‹è§£é‡ŠåŠ›ä¸è¶³ (RÂ² < 0.5)ï¼Œéœ€è¦é‡æ–°è®¾è®¡æ¨¡å‹")

        if 'val_mape' in key_metrics:
            mape = key_metrics['val_mape']
            if mape <= 10:
                recommendations.append("âœ… é¢„æµ‹ç²¾åº¦æé«˜ (MAPE â‰¤ 10%)")
            elif mape <= 20:
                recommendations.append("âœ… é¢„æµ‹ç²¾åº¦è‰¯å¥½ (10% < MAPE â‰¤ 20%)")
            elif mape <= 30:
                recommendations.append("âš ï¸  é¢„æµ‹ç²¾åº¦ä¸€èˆ¬ (20% < MAPE â‰¤ 30%)")
            else:
                recommendations.append("âŒ é¢„æµ‹ç²¾åº¦ä¸è¶³ (MAPE > 30%)")

        if 'training_history' in results and results['training_history']['overfitting_ratio'] > 1.5:
            recommendations.append("âš ï¸  æ£€æµ‹åˆ°è¿‡æ‹Ÿåˆé£é™©ï¼Œå»ºè®®ï¼šå¢åŠ æ­£åˆ™åŒ–ã€æ•°æ®å¢å¼ºã€æ—©åœ")
        elif 'training_history' in results and results['training_history']['overfitting_ratio'] < 1.1:
            recommendations.append("âš ï¸  å¯èƒ½æ¬ æ‹Ÿåˆï¼Œå»ºè®®ï¼šå¢åŠ æ¨¡å‹å¤æ‚åº¦ã€å»¶é•¿è®­ç»ƒæ—¶é—´")

        # æ‰“å°å»ºè®®
        print("\nğŸ“ è¯„ä¼°å»ºè®®:")
        for i, rec in enumerate(recommendations, 1):
            print(f"   {i}. {rec}")

        # æœ€ç»ˆè¯„çº§
        print(f"\nğŸ¯ æ¨¡å‹æ€»ä½“è¯„çº§:")
        good_count = sum(1 for rec in recommendations if 'âœ…' in rec)
        warn_count = sum(1 for rec in recommendations if 'âš ï¸' in rec)
        bad_count = sum(1 for rec in recommendations if 'âŒ' in rec)

        total_count = len(recommendations)

        if bad_count > 0:
            print("   âŒ éœ€è¦é‡å¤§æ”¹è¿›")
        elif warn_count > good_count:
            print("   âš ï¸  éœ€è¦ä¼˜åŒ–æ”¹è¿›")
        elif good_count >= total_count * 0.7:
            print("   âœ… æ€§èƒ½è‰¯å¥½ï¼Œå¯ä»¥è€ƒè™‘éƒ¨ç½²")
        else:
            print("   âš ï¸  æ€§èƒ½ä¸€èˆ¬ï¼Œå»ºè®®ä¼˜åŒ–")

        print("\n" + "=" * 80)
        print("ğŸ“ è¯„ä¼°å®Œæˆï¼æ‰€æœ‰ç»“æœå·²ä¿å­˜")
        print("=" * 80)

        results['recommendations'] = recommendations
        self.results = results

        return results


# ============================================================================
# ä½¿ç”¨ç¤ºä¾‹
# ============================================================================

def example_usage():
    """
    ä½¿ç”¨ç¤ºä¾‹
    """
    # å‡è®¾æ‚¨å·²ç»æœ‰äº†è®­ç»ƒå¥½çš„æ¨¡å‹å’Œè®­ç»ƒå™¨
    # model = YourTrainedModel()
    # trainer = ModelTrainer(model)
    # trainer.train(...)  # å·²ç»è®­ç»ƒå®Œæˆ

    # åˆ›å»ºè¯„ä¼°å™¨
    # evaluator = ModelEvaluator(model, trainer)

    # ç”Ÿæˆå®Œæ•´æŠ¥å‘Š
    # report = evaluator.generate_report(
    #     X_train=X_train_tensor,  # è®­ç»ƒæ•°æ®
    #     y_train=y_train_tensor,  # è®­ç»ƒæ ‡ç­¾
    #     X_val=X_val_tensor,      # éªŒè¯æ•°æ®
    #     y_val=y_val_tensor,      # éªŒè¯æ ‡ç­¾
    #     X_test=X_test_tensor,    # æµ‹è¯•æ•°æ®ï¼ˆå¯é€‰ï¼‰
    #     y_test=y_test_tensor,    # æµ‹è¯•æ ‡ç­¾ï¼ˆå¯é€‰ï¼‰
    #     feature_names=feature_names  # ç‰¹å¾åç§°ï¼ˆå¯é€‰ï¼‰
    # )

    print("ä½¿ç”¨æ–¹æ³•ï¼š")
    print("1. è®­ç»ƒå®Œæˆåï¼Œåˆ›å»ºModelEvaluatorå¯¹è±¡")
    print("2. è°ƒç”¨generate_report()æ–¹æ³•ç”Ÿæˆå®Œæ•´æŠ¥å‘Š")
    print("3. æŸ¥çœ‹æ§åˆ¶å°è¾“å‡ºå’Œå¯è§†åŒ–å›¾è¡¨")


if __name__ == "__main__":
    example_usage()