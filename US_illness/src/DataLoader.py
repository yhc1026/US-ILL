import torch
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings('ignore')

np.random.seed(42)


class FluDataset:
    def __init__(self, file_path, test_file_path):
        self.file_path = file_path
        self.test_file_path = test_file_path
        self.df_train = None
        self.df_test = None
        self.X = None
        self.y = None
        self.X_test = None
        self.scaler = StandardScaler()
        self.feature_names = []

    def load_data(self):
        self.df_train = pd.read_excel(self.file_path, engine='openpyxl')
        self.df_test = pd.read_excel(self.test_file_path, engine='openpyxl')
        print(f"è®­ç»ƒé›†åŠ è½½æˆåŠŸï¼Œå½¢çŠ¶: {self.df_train.shape}")
        print(f"æµ‹è¯•é›†åŠ è½½æˆåŠŸï¼Œå½¢çŠ¶: {self.df_test.shape}")

    def preprocess_data(self):
        print(f"\n=== æ•°æ®é¢„å¤„ç† ===")
        self.X, self.y = self._preprocess_data(self.df_train)
        self.X_test, y_waste= self._preprocess_data(self.df_test)

    def _preprocess_data(self, df):
        all_columns = list(df.columns)

        # 1. åˆ†ç¦»å·ç¼–ç  (åˆ—1-34)
        state_columns = all_columns[1:35]

        # 2. ç‰¹å¾ç»“æ„
        day1_start = 36
        day1_end = 53
        day2_start = 54
        day2_end = 71
        day3_start = 72
        day3_end = 89

        # 3. ç‰¹å¾å
        feature_names = [
            'cli', 'ili', 'wnohh_cmnty_cli', 'wbelief_masking_effective',
            'wbelief_distancing_effective', 'wcovid_vaccinated_friends',
            'wlarge_event_indoors', 'wothers_masked_public',
            'wothers_distanced_public', 'wshop_indoors',
            'wrestaurant_indoors', 'wworried_catch_covid',
            'hh_cmnty_cli', 'nohh_cmnty_cli', 'wearing_mask_7d',
            'public_transit', 'worried_finances', 'tested_positive'
        ]

        # 4. æ„å»ºç‰¹å¾Xå’Œç›®æ ‡y
        X_parts = []
        for idx in range(1, len(all_columns) - 1):
            col_name = all_columns[idx]
            X_parts.append(df[col_name].values.reshape(-1, 1))
        X = np.hstack(X_parts).astype(np.float32)
        target_col_name = all_columns[day3_end - 1]
        y = df[target_col_name].values.astype(np.float32)

        # # 6. æ„å»ºç‰¹å¾ååˆ—è¡¨
        # self.feature_names = []
        # self.feature_names.extend([f"state_{col}" for col in state_columns])
        # self.feature_names.extend([f"day1_{feature_names[i]}" for i in range(len(feature_names) - 1)])
        # self.feature_names.extend([f"day2_{feature_names[i]}" for i in range(len(feature_names) - 1)])

        print(f"è®­ç»ƒé›†ç‰¹å¾å½¢çŠ¶: {X.shape}")
        print(f"è®­ç»ƒé›†ç›®æ ‡å½¢çŠ¶: {y.shape}")
        print(f"è®­ç»ƒé›†ç›®æ ‡: Day3çš„ {target_col_name}")

        return X, y

    # def _preprocess_test_data(self, df):
    #     """é¢„å¤„ç†æµ‹è¯•æ•°æ®"""
    #     all_columns = list(df.columns)
    #
    #     # 1. åˆ†ç¦»å·ç¼–ç  (åˆ—1-34)
    #     state_columns = all_columns[1:35]
    #
    #     # 2. æµ‹è¯•é›†åªæœ‰å‰ä¸¤å¤©çš„å®Œæ•´æ•°æ® + ç¬¬ä¸‰å¤©çš„éƒ¨åˆ†æ•°æ®
    #     n_features_per_day = 17
    #
    #     # æ£€æŸ¥æµ‹è¯•é›†æ˜¯å¦æœ‰å®Œæ•´çš„Day3æ•°æ®
    #     total_cols = len(all_columns)
    #     expected_cols = 1 + 34 + 3 * n_features_per_day  # 86
    #
    #     if total_cols < expected_cols:
    #         # æµ‹è¯•é›†ç¼ºå°‘Day3çš„tested_positive
    #         print(f"æµ‹è¯•é›†åªæœ‰{total_cols}åˆ—ï¼Œç¼ºå°‘Day3çš„tested_positive")
    #
    #         # è®¡ç®—å®é™…çš„å¤©æ•°ç‰¹å¾
    #         available_cols = total_cols - 35  # å‡å»idå’Œå·ç¼–ç 
    #         # Day1 + Day2 = 34åˆ—ï¼Œå‰©ä½™çš„æ˜¯Day3çš„ç‰¹å¾
    #         day3_cols = available_cols - 34
    #         print(f"Day1: 17åˆ—, Day2: 17åˆ—, Day3: {day3_cols}åˆ—")
    #
    #     # 3. åˆ—ç´¢å¼•ï¼ˆæµ‹è¯•é›†å¯èƒ½æ²¡æœ‰å®Œæ•´çš„Day3ï¼‰
    #     day1_start = 35
    #     day1_end = day1_start + n_features_per_day
    #
    #     day2_start = day1_end
    #     day2_end = day2_start + n_features_per_day
    #
    #     # 4. æ„å»ºæµ‹è¯•ç‰¹å¾ X_testï¼ˆä¸è®­ç»ƒé›†æ ¼å¼ç›¸åŒï¼‰
    #     X_parts = []
    #
    #     # 4.1 å·ç¼–ç 
    #     X_parts.append(df[state_columns].values)
    #
    #     # 4.2 Day1ç‰¹å¾ï¼ˆå‰16ä¸ªï¼Œæ’é™¤tested_positiveï¼‰
    #     day1_features = []
    #     for i in range(day1_start, day1_end - 1):  # æ’é™¤æœ€åä¸€ä¸ª(tested_positive)
    #         col_name = all_columns[i]
    #         day1_features.append(df[col_name].values.reshape(-1, 1))
    #     day1_matrix = np.hstack(day1_features)
    #     X_parts.append(day1_matrix)
    #
    #     # 4.3 Day2ç‰¹å¾ï¼ˆå‰16ä¸ªï¼Œæ’é™¤tested_positiveï¼‰
    #     day2_features = []
    #     for i in range(day2_start, day2_end - 1):  # æ’é™¤æœ€åä¸€ä¸ª(tested_positive)
    #         col_name = all_columns[i]
    #         day2_features.append(df[col_name].values.reshape(-1, 1))
    #     day2_matrix = np.hstack(day2_features)
    #     X_parts.append(day2_matrix)
    #
    #     X_test = np.hstack(X_parts).astype(np.float32)
    #
    #     print(f"æµ‹è¯•é›†ç‰¹å¾å½¢çŠ¶: {X_test.shape}")
    #     print(f"æ³¨æ„: æµ‹è¯•é›†æ²¡æœ‰Day3çš„tested_positiveï¼Œéœ€è¦æ¨¡å‹é¢„æµ‹")
    #
    #     return X_test

    def create_datasets(self, val_size=0.2):
        # 1. åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
        X_train, X_val, y_train, y_val = train_test_split(
            self.X, self.y, test_size=val_size, random_state=42, shuffle=True
        )
        # pd.DataFrame(X_val).to_csv(r'D:\codeC\US_illness\output\val.csv', index=False)

        # 2. æ ‡å‡†åŒ–ç‰¹å¾ï¼ˆåªåœ¨è®­ç»ƒé›†ä¸Šfitï¼‰
        X_train_scaled = self.scaler.fit_transform(X_train)
        X_val_scaled = self.scaler.transform(X_val)
        X_test_scaled = self.scaler.transform(self.X_test)

        # 3. è½¬æ¢ä¸ºPyTorchå¼ é‡
        X_train_tensor = torch.FloatTensor(X_train_scaled)
        y_train_tensor = torch.FloatTensor(y_train).reshape(-1, 1)
        X_val_tensor = torch.FloatTensor(X_val_scaled)
        y_val_tensor = torch.FloatTensor(y_val).reshape(-1, 1)
        X_test_tensor = torch.FloatTensor(X_test_scaled)

        print(f"è®­ç»ƒé›†: {X_train_tensor.shape} ({(1 - val_size) * 100:.1f}%)")
        print(f"éªŒè¯é›†: {X_val_tensor.shape} ({val_size * 100:.1f}%)")
        print(f"æµ‹è¯•é›†: {X_test_tensor.shape}")
        print(f"æ€»è®­ç»ƒæ ·æœ¬æ•°: {len(self.X)}")
        print(f"è®­ç»ƒæ ·æœ¬æ•°: {len(X_train_tensor)}")
        print(f"éªŒè¯æ ·æœ¬æ•°: {len(X_val_tensor)}")
        print(f"æµ‹è¯•æ ·æœ¬æ•°: {len(X_test_tensor)}")

        return (X_train_tensor, y_train_tensor,
                X_val_tensor, y_val_tensor, X_test_tensor)

    def test(self):
        print("=== æ£€æŸ¥æµ‹è¯•é›†æ•°æ®æå– ===")
        print(f"æµ‹è¯•é›†å½¢çŠ¶: {self.df_test.shape}")
        print(f"æµ‹è¯•é›†åˆ—æ•°: {len(self.df_test.columns)}")
        print(f"æœ€åå‡ åˆ—å:")
        for i, col in enumerate(self.df_test.columns[-5:], 1):
            print(f"  åˆ—{i}: '{col}'")

        # æ£€æŸ¥ç‰¹å¾çŸ©é˜µ
        print(f"\nX_testå½¢çŠ¶: {self.X_test.shape}")
        print("X_testå‰3è¡Œå‰5åˆ—:")
        print(self.X_test[:3, :5])
        print("X_testå‰3è¡Œæœ€å5åˆ—:")
        print(self.X_test[:3, -5:])
        print("=== è®­ç»ƒé›†ç›®æ ‡å˜é‡åˆ†æ ===")
        y_train_vals = self.y  # ä½ çš„è®­ç»ƒé›†ç›®æ ‡

        print(f"è®­ç»ƒé›†yç»Ÿè®¡:")
        print(f"  æœ€å°å€¼: {y_train_vals.min():.4f}")
        print(f"  æœ€å¤§å€¼: {y_train_vals.max():.4f}")
        print(f"  å‡å€¼: {y_train_vals.mean():.4f}")
        print(f"  ä¸­ä½æ•°: {np.median(y_train_vals):.4f}")
        print(f"  æ ‡å‡†å·®: {y_train_vals.std():.4f}")

        # æŸ¥çœ‹åˆ†å¸ƒ
        plt.figure(figsize=(10, 4))
        plt.hist(y_train_vals, bins=50, edgecolor='black', alpha=0.7)
        plt.axvline(y_train_vals.mean(), color='red', linestyle='--', label=f'å‡å€¼={y_train_vals.mean():.2f}')
        plt.axvline(np.median(y_train_vals), color='green', linestyle='--',
                    label=f'ä¸­ä½æ•°={np.median(y_train_vals):.2f}')
        plt.xlabel('tested_positive')
        plt.ylabel('é¢‘æ¬¡')
        plt.title('è®­ç»ƒé›†ç›®æ ‡å˜é‡åˆ†å¸ƒ')
        plt.legend()
        plt.show()
        wi_col = 'WI'  # æ ¹æ®ä½ çš„åˆ—åè°ƒæ•´
        if wi_col in self.df_train.columns:
            wi_mask = self.df_train[wi_col] == 1
            wi_samples = wi_mask.sum()
            print(f"\nè®­ç»ƒé›†ä¸­WIå·æ ·æœ¬æ•°: {wi_samples}/{len(self.df_train)}")

            if wi_samples > 0:
                wi_y = self.y[wi_mask]
                print(f"WIå·çš„ç›®æ ‡å˜é‡ç»Ÿè®¡:")
                print(f"  èŒƒå›´: [{wi_y.min():.2f}, {wi_y.max():.2f}]")
                print(f"  å‡å€¼: {wi_y.mean():.2f}")

    def check_data_leakage_in_detail(dataset):
        """è¯¦ç»†æ£€æŸ¥æ•°æ®æ³„éœ²"""

        print("ğŸ” è¯¦ç»†æ£€æŸ¥æ•°æ®æ³„éœ²")
        print("=" * 60)

        # 1. æŸ¥çœ‹åˆ—ç»“æ„
        print("1. è®­ç»ƒé›†åˆ—ç»“æ„:")
        train_cols = list(dataset.df_train.columns)
        print(f"   æ€»åˆ—æ•°: {len(train_cols)}")
        print(f"   å‰5åˆ—: {train_cols[:5]}")
        print(f"   æœ€å5åˆ—: {train_cols[-5:]}")

        # 2. æ£€æŸ¥ç›®æ ‡åˆ—ä½ç½®
        day3_end = 89  # æ ¹æ®æ‚¨çš„ä»£ç 
        target_idx = day3_end - 1
        print(f"\n2. ç›®æ ‡åˆ—ä½ç½®æ£€æŸ¥:")
        print(f"   ç›®æ ‡åˆ—ç´¢å¼•: {target_idx}")
        print(f"   ç›®æ ‡åˆ—å: '{train_cols[target_idx]}'")

        # 3. æ£€æŸ¥ç‰¹å¾åŒ…å«çš„åˆ—èŒƒå›´
        print(f"\n3. ç‰¹å¾åŒ…å«çš„åˆ—èŒƒå›´:")
        print(f"   ç‰¹å¾å¾ªç¯: range(1, {len(train_cols)} - 1)")
        print(f"   å®é™…èŒƒå›´: åˆ—ç´¢å¼• 1 åˆ° {len(train_cols) - 2}")
        print(f"   è¿™æ„å‘³ç€ç‰¹å¾åŒ…å«äº†åˆ—: {train_cols[1]} åˆ° '{train_cols[-2]}'")

        # 4. å…³é”®æ£€æŸ¥ï¼šç›®æ ‡åˆ—æ˜¯å¦åœ¨ç‰¹å¾ä¸­
        if target_idx < len(train_cols) - 1:
            print(f"\nâŒâŒâŒ ä¸¥é‡æ•°æ®æ³„éœ²ï¼âŒâŒâŒ")
            print(f"   ç›®æ ‡åˆ—ç´¢å¼•: {target_idx}")
            print(f"   ç‰¹å¾åŒ…å«åˆ°åˆ—ç´¢å¼•: {len(train_cols) - 2}")
            print(f"   ç›®æ ‡åˆ— '{train_cols[target_idx]}' è¢«åŒ…å«åœ¨ç‰¹å¾ä¸­ï¼")
            print(f"   è¿™ç­‰äºåœ¨è€ƒè¯•æ—¶ç›´æ¥æŠŠç­”æ¡ˆç»™äº†æ¨¡å‹ï¼")
            return True
        else:
            print(f"\nâœ… ç›®æ ‡åˆ—ä¸åœ¨ç‰¹å¾ä¸­")
            return False

if __name__ == "__main__":
    dataset=FluDataset(r"D:\codeC\US_illness\data\train.xlsx",r"D:\codeC\US_illness\data\test.xlsx")
    dataset.load_data()
    dataset.preprocess_data()
    dataset.create_datasets()
    dataset.check_data_leakage_in_detail()
#    dataset.test()

